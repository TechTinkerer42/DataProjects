{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction:-NLP-Learning-for-Job-Descriptions\" data-toc-modified-id=\"Introduction:-NLP-Learning-for-Job-Descriptions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction: NLP Learning for Job Descriptions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Python-Library\" data-toc-modified-id=\"Python-Library-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Python Library</a></span></li></ul></li><li><span><a href=\"#Data-Set-Loading-and-Cleaning-Up\" data-toc-modified-id=\"Data-Set-Loading-and-Cleaning-Up-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Set Loading and Cleaning Up</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Job-Description-CSV-Data\" data-toc-modified-id=\"Load-Job-Description-CSV-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load Job Description CSV Data</a></span></li><li><span><a href=\"#Clean-Up\" data-toc-modified-id=\"Clean-Up-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Clean Up</a></span></li></ul></li><li><span><a href=\"#Text-Feature-Engineering\" data-toc-modified-id=\"Text-Feature-Engineering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Text Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Tokenization</a></span></li><li><span><a href=\"#Stopword-Removal\" data-toc-modified-id=\"Stopword-Removal-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Stopword Removal</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Lemmatization</a></span></li><li><span><a href=\"#Word-Embedding-Vectors-with-Gensim\" data-toc-modified-id=\"Word-Embedding-Vectors-with-Gensim-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Word Embedding Vectors with Gensim</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word2Vec-Mean-Vector\" data-toc-modified-id=\"Word2Vec-Mean-Vector-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Word2Vec Mean Vector</a></span></li><li><span><a href=\"#Covert-Vectors-into-Columns\" data-toc-modified-id=\"Covert-Vectors-into-Columns-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Covert Vectors into Columns</a></span></li></ul></li></ul></li><li><span><a href=\"#Training-and-Testing-Data-Preparation\" data-toc-modified-id=\"Training-and-Testing-Data-Preparation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training and Testing Data Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-Scaling\" data-toc-modified-id=\"Standard-Scaling-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Standard Scaling</a></span></li><li><span><a href=\"#Label-Encoding\" data-toc-modified-id=\"Label-Encoding-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Label Encoding</a></span></li><li><span><a href=\"#Data-Splits-for-Training-and-Testing-Data-Sets\" data-toc-modified-id=\"Data-Splits-for-Training-and-Testing-Data-Sets-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Data Splits for Training and Testing Data Sets</a></span></li></ul></li><li><span><a href=\"#Neural-Network-Classification\" data-toc-modified-id=\"Neural-Network-Classification-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Neural Network Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Neural-Network-Multi-class-Classifier-Training\" data-toc-modified-id=\"Neural-Network-Multi-class-Classifier-Training-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Neural Network Multi-class Classifier Training</a></span></li><li><span><a href=\"#Training-Validation\" data-toc-modified-id=\"Training-Validation-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Training Validation</a></span></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: NLP Learning for Job Descriptions\n",
    "\n",
    "In this notebook, I will demonstrate how to process unstructure text feature data by using natural language processing (NLP) technologies and train a classification model with those text features. I will use NLTK and gensim word2vec to do text feature engineering and then use multi-class neural network classifier to train a classification model.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset is a historical data of job descriptions stored as \"job_descriptions.csv\" file.\n",
    "\n",
    "## Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:00:22.525455Z",
     "start_time": "2020-01-20T20:00:18.984203Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas and numpy for converting from Spark dataframe into Pandas dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Make the random numbers predictable\n",
    "np.random.seed(42)\n",
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:00:25.167027Z",
     "start_time": "2020-01-20T20:00:25.164159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Allow multiple output/display from one cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:00:28.586392Z",
     "start_time": "2020-01-20T20:00:26.929901Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "# Stop Word Removal\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:00:31.056460Z",
     "start_time": "2020-01-20T20:00:30.967710Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:00:39.386018Z",
     "start_time": "2020-01-20T20:00:33.997229Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set Loading and Cleaning Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Job Description CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:00:43.969095Z",
     "start_time": "2020-01-20T20:00:42.829963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 127784 entries, 0 to 127783\n",
      "Data columns (total 8 columns):\n",
      "req_guid                  127784 non-null object\n",
      "original_hcs_code         127784 non-null object\n",
      "original_hcs_level        127784 non-null object\n",
      "updated_assigned_hcs      127784 non-null object\n",
      "updated_assigned_level    127784 non-null int64\n",
      "level_indicator           127784 non-null int64\n",
      "job_title                 127783 non-null object\n",
      "job_description           127784 non-null object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 7.8+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3051: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./job_descriptions.csv', header='infer')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:00:47.095080Z",
     "start_time": "2020-01-20T20:00:47.092135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total amount of training data on job descriptions is:  127784\n"
     ]
    }
   ],
   "source": [
    "print(\"The total amount of training data on job descriptions is: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:00:51.460173Z",
     "start_time": "2020-01-20T20:00:51.393368Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove any rows without job_description\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:00:54.232042Z",
     "start_time": "2020-01-20T20:00:54.007112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine simply job description and title so that job title is a part of job description\n",
    "df['job_description'] = df['job_title'] + \" \" + df['job_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:00:56.059820Z",
     "start_time": "2020-01-20T20:00:56.043725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare final data set for next step of text feature engineering\n",
    "df = df[['req_guid', 'updated_assigned_hcs', 'job_title', 'job_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:00:58.445925Z",
     "start_time": "2020-01-20T20:00:58.383317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 127783 entries, 0 to 127783\n",
      "Data columns (total 4 columns):\n",
      "req_guid                127783 non-null object\n",
      "updated_assigned_hcs    127783 non-null object\n",
      "job_title               127783 non-null object\n",
      "job_description         127783 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 4.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>req_guid</th>\n",
       "      <th>updated_assigned_hcs</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15795912</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Warehouse Material Handler 1st Shift TEMP 6 mo</td>\n",
       "      <td>Warehouse Material Handler 1st Shift TEMP 6 mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15370412</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Structural Welder</td>\n",
       "      <td>Structural Welder Welding structural steel wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15735158</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Construction Laborer</td>\n",
       "      <td>Construction Laborer This individual will be r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15797256</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Wrapping</td>\n",
       "      <td>Wrapping Must Have Ability to lift 50lbs palle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15469997</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Operations Representative</td>\n",
       "      <td>Operations Representative Executes the busines...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   req_guid updated_assigned_hcs  \\\n",
       "0  15795912               00-000   \n",
       "1  15370412               00-000   \n",
       "2  15735158               00-000   \n",
       "3  15797256               00-000   \n",
       "4  15469997               00-000   \n",
       "\n",
       "                                         job_title  \\\n",
       "0  Warehouse Material Handler 1st Shift TEMP 6 mo    \n",
       "1                                Structural Welder   \n",
       "2                             Construction Laborer   \n",
       "3                                         Wrapping   \n",
       "4                        Operations Representative   \n",
       "\n",
       "                                     job_description  \n",
       "0  Warehouse Material Handler 1st Shift TEMP 6 mo...  \n",
       "1  Structural Welder Welding structural steel wit...  \n",
       "2  Construction Laborer This individual will be r...  \n",
       "3  Wrapping Must Have Ability to lift 50lbs palle...  \n",
       "4  Operations Representative Executes the busines...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Engineering\n",
    "\n",
    "In this step, raw text data will be transformed into feature vectors. I will implement the following steps in order to obtain relevant features from the dataset.\n",
    "\n",
    "* Tokenizing\n",
    "* Remove stop words\n",
    "* Lemmatization (not stem since stemming can reduce the interpretability) \n",
    "* Word Embeddings as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process by dividing the quantity of text into smaller parts called tokens so that each token can be further treated for machine learning purposes. A token can be a character, a word, a sentence or a paragraph. In this notebook, I only consider words as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:01:08.926694Z",
     "start_time": "2020-01-20T20:01:05.685348Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the job description and title\n",
    "# I can use NLTK word_tokenize function to process the job description field (by removing punctuations \n",
    "# and separating words) like below\n",
    "df['job_description'] = df.apply(lambda row: word_tokenize(row.job_description), axis=1)\n",
    "# Or I can just use python string split function to separate text since the job description has been cleaned\n",
    "df['job_description'] = df[\"job_description\"].str.lower()\n",
    "df['job_description'] = df[\"job_description\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:01:47.949226Z",
     "start_time": "2020-01-20T20:01:47.934791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>req_guid</th>\n",
       "      <th>updated_assigned_hcs</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15795912</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Warehouse Material Handler 1st Shift TEMP 6 mo</td>\n",
       "      <td>[warehouse, material, handler, 1st, shift, tem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15370412</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Structural Welder</td>\n",
       "      <td>[structural, welder, welding, structural, stee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15735158</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Construction Laborer</td>\n",
       "      <td>[construction, laborer, this, individual, will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15797256</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Wrapping</td>\n",
       "      <td>[wrapping, must, have, ability, to, lift, 50lb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15469997</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Operations Representative</td>\n",
       "      <td>[operations, representative, executes, the, bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   req_guid updated_assigned_hcs  \\\n",
       "0  15795912               00-000   \n",
       "1  15370412               00-000   \n",
       "2  15735158               00-000   \n",
       "3  15797256               00-000   \n",
       "4  15469997               00-000   \n",
       "\n",
       "                                         job_title  \\\n",
       "0  Warehouse Material Handler 1st Shift TEMP 6 mo    \n",
       "1                                Structural Welder   \n",
       "2                             Construction Laborer   \n",
       "3                                         Wrapping   \n",
       "4                        Operations Representative   \n",
       "\n",
       "                                     job_description  \n",
       "0  [warehouse, material, handler, 1st, shift, tem...  \n",
       "1  [structural, welder, welding, structural, stee...  \n",
       "2  [construction, laborer, this, individual, will...  \n",
       "3  [wrapping, must, have, ability, to, lift, 50lb...  \n",
       "4  [operations, representative, executes, the, bu...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a NLP program has been programmed to ignore. In this notebook, I will use NLTK stop words dataset to remove any stop words in job description field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:06:24.662103Z",
     "start_time": "2020-01-20T20:05:57.264534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>req_guid</th>\n",
       "      <th>updated_assigned_hcs</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15795912</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Warehouse Material Handler 1st Shift TEMP 6 mo</td>\n",
       "      <td>[warehouse, material, handler, 1st, shift, tem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15370412</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Structural Welder</td>\n",
       "      <td>[structural, welder, welding, structural, stee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15735158</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Construction Laborer</td>\n",
       "      <td>[construction, laborer, individual, responsibl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15797256</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Wrapping</td>\n",
       "      <td>[wrapping, must, ability, lift, 50lbs, palleti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15469997</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Operations Representative</td>\n",
       "      <td>[operations, representative, executes, busines...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   req_guid updated_assigned_hcs  \\\n",
       "0  15795912               00-000   \n",
       "1  15370412               00-000   \n",
       "2  15735158               00-000   \n",
       "3  15797256               00-000   \n",
       "4  15469997               00-000   \n",
       "\n",
       "                                         job_title  \\\n",
       "0  Warehouse Material Handler 1st Shift TEMP 6 mo    \n",
       "1                                Structural Welder   \n",
       "2                             Construction Laborer   \n",
       "3                                         Wrapping   \n",
       "4                        Operations Representative   \n",
       "\n",
       "                                     job_description  \n",
       "0  [warehouse, material, handler, 1st, shift, tem...  \n",
       "1  [structural, welder, welding, structural, stee...  \n",
       "2  [construction, laborer, individual, responsibl...  \n",
       "3  [wrapping, must, ability, lift, 50lbs, palleti...  \n",
       "4  [operations, representative, executes, busines...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stopwords list from NLTK library\n",
    "stop_words = stopwords.words('english')\n",
    "# Define a function to remove any stop words from input text\n",
    "def removeStopWords(x):\n",
    "        return [w.lower() for w in x if (w not in stop_words) and (w != '') and (w is not None)]\n",
    "# Apply the defined function to remove stop words for job descriptions\n",
    "df['job_description'] = df.apply(lambda row: removeStopWords(row.job_description), axis=1)\n",
    "# Show some results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. For example, in English, the verb 'to walk' may appear as 'walk', 'walked', 'walks', 'walking'. The base form, 'walk', that one might look up in a dictionary, is called the lemma for the word. I will use NLTK lemmatization function to convert words into their lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:10:30.933994Z",
     "start_time": "2020-01-20T20:09:36.235121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>req_guid</th>\n",
       "      <th>updated_assigned_hcs</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15795912</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Warehouse Material Handler 1st Shift TEMP 6 mo</td>\n",
       "      <td>[warehouse, material, handler, 1st, shift, tem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15370412</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Structural Welder</td>\n",
       "      <td>[structural, welder, weld, structural, steel, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15735158</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Construction Laborer</td>\n",
       "      <td>[construction, laborer, individual, responsibl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15797256</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Wrapping</td>\n",
       "      <td>[wrap, must, ability, lift, 50lbs, palletizing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15469997</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Operations Representative</td>\n",
       "      <td>[operations, representative, execute, business...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   req_guid updated_assigned_hcs  \\\n",
       "0  15795912               00-000   \n",
       "1  15370412               00-000   \n",
       "2  15735158               00-000   \n",
       "3  15797256               00-000   \n",
       "4  15469997               00-000   \n",
       "\n",
       "                                         job_title  \\\n",
       "0  Warehouse Material Handler 1st Shift TEMP 6 mo    \n",
       "1                                Structural Welder   \n",
       "2                             Construction Laborer   \n",
       "3                                         Wrapping   \n",
       "4                        Operations Representative   \n",
       "\n",
       "                                     job_description  \n",
       "0  [warehouse, material, handler, 1st, shift, tem...  \n",
       "1  [structural, welder, weld, structural, steel, ...  \n",
       "2  [construction, laborer, individual, responsibl...  \n",
       "3  [wrap, must, ability, lift, 50lbs, palletizing...  \n",
       "4  [operations, representative, execute, business...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define lemmatization function by using NLTK WordNetLemmatizer function\n",
    "def lemma(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w.lower(), pos='v') for w in x if (w != '') and (w is not None)]\n",
    "# Apply the defined function to process job descriptions\n",
    "df['job_description'] = df.apply(lambda row: lemma(row.job_description), axis=1)\n",
    "# Show some results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Vectors with Gensim\n",
    "\n",
    "A word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be trained using the input texts. One can read more about word embeddings [here](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/) and [here](https://jalammar.github.io/illustrated-word2vec/).\n",
    "\n",
    "In this notebook, I am going to use gensim library Word2Vec functionality to generate word embedding vectors so that I can use those vectors later on to train the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Mean Vector\n",
    "\n",
    "Gensim Word2Vec will generate a vector (dimension of 100 here) for each word after training based on all job descriptions. So, I will define a function to get average (mean) vectors for a job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:19:20.302268Z",
     "start_time": "2020-01-20T20:19:20.287578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare all the text input for training word2vec model\n",
    "sentences = df['job_description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:23:15.962320Z",
     "start_time": "2020-01-20T20:22:27.722221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=55287, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Define and train a word2vec model. \n",
    "# Here I set vector dimension size to be 100, window (word distanse) to be 5 \n",
    "# and use all available CPUs for parallel processing\n",
    "model_w2v = Word2Vec(sentences, size=100, window=5, min_count=1, workers=cpu_count)\n",
    "# summarize vocabulary\n",
    "# word_vocabulary = list(model_w2v.wv.vocab)\n",
    "# print(word_vocabulary)\n",
    "# save model with binary format\n",
    "model_w2v.save('nnc_word2vec.pkl')\n",
    "# load model when needed so that this word2vec model doesn't need to be re-trained\n",
    "# model_w2v = Word2Vec.load('nnc_word2vec.pkl')\n",
    "print(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:23:19.818943Z",
     "start_time": "2020-01-20T20:23:19.815181Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to get average (mean) vectors for a job description based on trained word2vec model\n",
    "def get_mean_vectors(words):\n",
    "    # remove out of vocabulary words\n",
    "    words = [word for word in words if word in model_w2v.wv.vocab]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(model_w2v[words], axis=0)\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:25:14.789111Z",
     "start_time": "2020-01-20T20:24:23.307338Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>req_guid</th>\n",
       "      <th>updated_assigned_hcs</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "      <th>job_description_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15795912</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Warehouse Material Handler 1st Shift TEMP 6 mo</td>\n",
       "      <td>[warehouse, material, handler, 1st, shift, tem...</td>\n",
       "      <td>[0.067955986, -0.052602034, 0.108059414, -0.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15370412</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Structural Welder</td>\n",
       "      <td>[structural, welder, weld, structural, steel, ...</td>\n",
       "      <td>[1.5541024, -1.2365535, -0.10899881, 1.2434216...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15735158</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Construction Laborer</td>\n",
       "      <td>[construction, laborer, individual, responsibl...</td>\n",
       "      <td>[-0.167428, -0.9671807, 0.015980506, -0.235558...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15797256</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Wrapping</td>\n",
       "      <td>[wrap, must, ability, lift, 50lbs, palletizing...</td>\n",
       "      <td>[-0.2389299, 0.036015894, -0.20679249, -0.1900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15469997</td>\n",
       "      <td>00-000</td>\n",
       "      <td>Operations Representative</td>\n",
       "      <td>[operations, representative, execute, business...</td>\n",
       "      <td>[0.035469715, -0.04396796, -0.034893647, 0.377...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   req_guid updated_assigned_hcs  \\\n",
       "0  15795912               00-000   \n",
       "1  15370412               00-000   \n",
       "2  15735158               00-000   \n",
       "3  15797256               00-000   \n",
       "4  15469997               00-000   \n",
       "\n",
       "                                         job_title  \\\n",
       "0  Warehouse Material Handler 1st Shift TEMP 6 mo    \n",
       "1                                Structural Welder   \n",
       "2                             Construction Laborer   \n",
       "3                                         Wrapping   \n",
       "4                        Operations Representative   \n",
       "\n",
       "                                     job_description  \\\n",
       "0  [warehouse, material, handler, 1st, shift, tem...   \n",
       "1  [structural, welder, weld, structural, steel, ...   \n",
       "2  [construction, laborer, individual, responsibl...   \n",
       "3  [wrap, must, ability, lift, 50lbs, palletizing...   \n",
       "4  [operations, representative, execute, business...   \n",
       "\n",
       "                             job_description_vectors  \n",
       "0  [0.067955986, -0.052602034, 0.108059414, -0.27...  \n",
       "1  [1.5541024, -1.2365535, -0.10899881, 1.2434216...  \n",
       "2  [-0.167428, -0.9671807, 0.015980506, -0.235558...  \n",
       "3  [-0.2389299, 0.036015894, -0.20679249, -0.1900...  \n",
       "4  [0.035469715, -0.04396796, -0.034893647, 0.377...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to generate 100 dimension vectors for each job description in the data set\n",
    "df['job_description_vectors'] = df.apply(lambda row: get_mean_vectors(row.job_description), axis=1) \n",
    "# Show some sample results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, I can use gensim library Doc2Vec function to generate vectors as well.  However, I don't choose this method due to relatively too large files that Doc2Vec function will produce (see the screen shot below). Any interested in the discussion on Word2Vec vs. Doc2Vec, it can be refer to this [link](https://datascience.stackexchange.com/questions/20076/word2vec-vs-sentence2vec-vs-doc2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Title 1](./Capture.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-19T00:30:01.745857Z",
     "start_time": "2020-01-19T00:30:01.743692Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare tagged documents for training\n",
    "descriptions = df['job_description'].tolist()\n",
    "tags = df['req_guid'].tolist()\n",
    "docs = []\n",
    "for i in range(len(tags)):\n",
    "    docs.append(doc2vec.TaggedDocument(words=descriptions[i], tags=[\"Train-\" + str(tags[i])]))\n",
    "    \n",
    "docs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-19T00:30:01.769771Z",
     "start_time": "2020-01-19T00:30:01.762470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup doc2vec model which is similar to word2vec model setup\n",
    "model_d2v = doc2vec.Doc2Vec(dm=0, vector_size=100, window=5, min_count=1, workers=cpu_count)\n",
    "# summarize vocabulary\n",
    "model_d2v.build_vocab(docs)\n",
    "# train model\n",
    "model_d2v.train(docs, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)\n",
    "# save model with binary format\n",
    "model_d2v.save('doc2vec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-19T00:30:01.780067Z",
     "start_time": "2020-01-19T00:30:01.772960Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load doc2vec model when needed so that the model don't have to be re-trained\n",
    "model_d2v = doc2vec.Doc2Vec.load('doc2vec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-19T00:30:01.793032Z",
     "start_time": "2020-01-19T00:30:01.788138Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the job description vectors from the trained doc2vec model by using infer_vector function\n",
    "df['job_description_vectors'] = df.apply(lambda row: model_d2v.infer_vector(row.job_description), axis=1)\n",
    "# or directly retrieve from tagged documents\n",
    "df['job_description_vectors'] = df.apply(lambda row: model_d2v.docvecs[\"Train-\" + str(row.req_guid)], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covert Vectors into Columns\n",
    "\n",
    "Here, I am going to convert job description vectors into columns so that I can easily generate training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:35:02.146207Z",
     "start_time": "2020-01-20T20:35:01.453526Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Put job description vectors as columns\n",
    "series = df['job_description_vectors'].apply(lambda x : np.array(x)).as_matrix().reshape(-1,1)\n",
    "w2v = np.apply_along_axis(lambda x : x[0], 1, series)\n",
    "w2v_df = pd.DataFrame(w2v)\n",
    "final_df = pd.concat([df, w2v_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:35:06.437910Z",
     "start_time": "2020-01-20T20:35:06.288614Z"
    }
   },
   "outputs": [],
   "source": [
    "# Name label field so that the classification model knows which field is the target (label) one\n",
    "final_df.rename(columns={'updated_assigned_hcs':'label'}, inplace=True)\n",
    "# Prepare final data table by removing unuseful columns\n",
    "final_df.drop(columns=['req_guid','job_title', 'job_description', 'job_description_vectors'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:35:12.700244Z",
     "start_time": "2020-01-20T20:35:12.567306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove any NULL or blank rows\n",
    "final_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:35:34.655536Z",
     "start_time": "2020-01-20T20:35:34.630405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-000</td>\n",
       "      <td>0.067956</td>\n",
       "      <td>-0.052602</td>\n",
       "      <td>0.108059</td>\n",
       "      <td>-0.274452</td>\n",
       "      <td>-0.445171</td>\n",
       "      <td>0.184035</td>\n",
       "      <td>-0.415023</td>\n",
       "      <td>-0.282931</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276811</td>\n",
       "      <td>-0.602863</td>\n",
       "      <td>0.552518</td>\n",
       "      <td>0.552918</td>\n",
       "      <td>-0.268735</td>\n",
       "      <td>0.196824</td>\n",
       "      <td>-0.345696</td>\n",
       "      <td>0.906765</td>\n",
       "      <td>-0.207538</td>\n",
       "      <td>-0.412793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-000</td>\n",
       "      <td>1.554102</td>\n",
       "      <td>-1.236554</td>\n",
       "      <td>-0.108999</td>\n",
       "      <td>1.243422</td>\n",
       "      <td>-1.273140</td>\n",
       "      <td>-0.707512</td>\n",
       "      <td>0.086695</td>\n",
       "      <td>1.114845</td>\n",
       "      <td>0.540642</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.784990</td>\n",
       "      <td>-0.105541</td>\n",
       "      <td>0.763936</td>\n",
       "      <td>-1.061563</td>\n",
       "      <td>0.216444</td>\n",
       "      <td>-0.514710</td>\n",
       "      <td>0.317936</td>\n",
       "      <td>1.307202</td>\n",
       "      <td>-1.117071</td>\n",
       "      <td>0.313168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00-000</td>\n",
       "      <td>-0.167428</td>\n",
       "      <td>-0.967181</td>\n",
       "      <td>0.015981</td>\n",
       "      <td>-0.235559</td>\n",
       "      <td>-0.347159</td>\n",
       "      <td>-0.294594</td>\n",
       "      <td>-0.496681</td>\n",
       "      <td>0.486209</td>\n",
       "      <td>-0.905325</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.789382</td>\n",
       "      <td>-0.309183</td>\n",
       "      <td>0.445784</td>\n",
       "      <td>0.726108</td>\n",
       "      <td>0.158220</td>\n",
       "      <td>-0.923460</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>-0.397063</td>\n",
       "      <td>-1.572622</td>\n",
       "      <td>0.203792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00-000</td>\n",
       "      <td>-0.238930</td>\n",
       "      <td>0.036016</td>\n",
       "      <td>-0.206792</td>\n",
       "      <td>-0.190075</td>\n",
       "      <td>-1.207528</td>\n",
       "      <td>0.260064</td>\n",
       "      <td>-0.293271</td>\n",
       "      <td>0.376710</td>\n",
       "      <td>-0.099776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576839</td>\n",
       "      <td>-1.660701</td>\n",
       "      <td>0.790664</td>\n",
       "      <td>-0.204595</td>\n",
       "      <td>-0.127094</td>\n",
       "      <td>-1.202703</td>\n",
       "      <td>0.254096</td>\n",
       "      <td>1.396883</td>\n",
       "      <td>0.242860</td>\n",
       "      <td>0.493747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00-000</td>\n",
       "      <td>0.035470</td>\n",
       "      <td>-0.043968</td>\n",
       "      <td>-0.034894</td>\n",
       "      <td>0.377821</td>\n",
       "      <td>-0.157486</td>\n",
       "      <td>0.530797</td>\n",
       "      <td>-0.689256</td>\n",
       "      <td>-1.000499</td>\n",
       "      <td>0.215361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117709</td>\n",
       "      <td>0.898091</td>\n",
       "      <td>0.442654</td>\n",
       "      <td>0.131698</td>\n",
       "      <td>0.141214</td>\n",
       "      <td>0.057979</td>\n",
       "      <td>0.185774</td>\n",
       "      <td>-0.812538</td>\n",
       "      <td>0.350675</td>\n",
       "      <td>0.269515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label         0         1         2         3         4         5  \\\n",
       "0  00-000  0.067956 -0.052602  0.108059 -0.274452 -0.445171  0.184035   \n",
       "1  00-000  1.554102 -1.236554 -0.108999  1.243422 -1.273140 -0.707512   \n",
       "2  00-000 -0.167428 -0.967181  0.015981 -0.235559 -0.347159 -0.294594   \n",
       "3  00-000 -0.238930  0.036016 -0.206792 -0.190075 -1.207528  0.260064   \n",
       "4  00-000  0.035470 -0.043968 -0.034894  0.377821 -0.157486  0.530797   \n",
       "\n",
       "          6         7         8  ...        90        91        92        93  \\\n",
       "0 -0.415023 -0.282931  0.242700  ...  0.276811 -0.602863  0.552518  0.552918   \n",
       "1  0.086695  1.114845  0.540642  ... -0.784990 -0.105541  0.763936 -1.061563   \n",
       "2 -0.496681  0.486209 -0.905325  ... -0.789382 -0.309183  0.445784  0.726108   \n",
       "3 -0.293271  0.376710 -0.099776  ... -0.576839 -1.660701  0.790664 -0.204595   \n",
       "4 -0.689256 -1.000499  0.215361  ...  0.117709  0.898091  0.442654  0.131698   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0 -0.268735  0.196824 -0.345696  0.906765 -0.207538 -0.412793  \n",
       "1  0.216444 -0.514710  0.317936  1.307202 -1.117071  0.313168  \n",
       "2  0.158220 -0.923460  0.015126 -0.397063 -1.572622  0.203792  \n",
       "3 -0.127094 -1.202703  0.254096  1.396883  0.242860  0.493747  \n",
       "4  0.141214  0.057979  0.185774 -0.812538  0.350675  0.269515  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some sample results\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:39:55.406603Z",
     "start_time": "2020-01-20T20:39:55.274159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get numpy array on feature set X and target set y\n",
    "X = np.array(final_df.drop(columns=['label']))\n",
    "\n",
    "y = np.array(final_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:40:10.183124Z",
     "start_time": "2020-01-20T20:40:10.178484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127782, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(127782,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check their shape\n",
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaling\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. neural network classifier). People usually can standardize features by removing the mean and scaling to unit variance. In this notebook, I will use StandardScaler function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:42:51.712948Z",
     "start_time": "2020-01-20T20:42:51.386382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.147318  ,  0.08925455,  0.09173775, ...,  0.8263443 ,\n",
       "         0.22614478, -0.7334884 ],\n",
       "       [ 3.0366647 , -2.2223854 , -0.5396993 , ...,  1.3645416 ,\n",
       "        -1.4959618 ,  0.7712565 ],\n",
       "       [-0.65161455, -1.6964407 , -0.17612602, ..., -0.9260334 ,\n",
       "        -2.3585    ,  0.5445462 ],\n",
       "       ...,\n",
       "       [ 1.2356126 , -2.3783293 , -0.24089916, ...,  0.2772672 ,\n",
       "        -0.22560264,  1.4271886 ],\n",
       "       [ 0.23332882,  1.2109891 ,  0.48697844, ..., -0.3224733 ,\n",
       "        -0.06261613,  2.0399702 ],\n",
       "       [-0.30823442,  1.4374849 , -1.8495783 , ..., -2.1731262 ,\n",
       "         2.2189806 , -1.0544403 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardize the vectors\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:42:55.609126Z",
     "start_time": "2020-01-20T20:42:55.605328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00-000', '00-000', '00-000', ..., '00-000', '00-000', '00-000'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "\n",
    "Holds the label for each class. Encode categorical features using a one-hot or ordinal encoding scheme. It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. In this notebook, I am going to use LabelEncoder function to generate indexes for the target label field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:45:39.439047Z",
     "start_time": "2020-01-20T20:45:39.410464Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding the lebels\n",
    "tmp = y\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:45:46.892294Z",
     "start_time": "2020-01-20T20:45:46.866591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the mapping between index and labels so that I can refer back to labels when get predicted results.\n",
    "df_label = pd.DataFrame({'label':tmp, 'label_index':y})\n",
    "df_label.drop_duplicates().to_csv('./hcs_label_index.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splits for Training and Testing Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:47:09.554018Z",
     "start_time": "2020-01-20T20:47:09.406354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102225\n",
      "25557\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Multi-class Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:52:17.642846Z",
     "start_time": "2020-01-20T20:52:17.637729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a neural netowork multi-class classifier model\n",
    "# Two hidden layers are set to 100 and 85\n",
    "# Activation function is set to relu\n",
    "# Solver function is set to adam\n",
    "# Learning rate is set to adaptive, which means it will automatically adjust from initial rate\n",
    "# Early stopping is set to true so that it won't waste time when not much improvement can be achieved\n",
    "# Alpha is set to 0.01 so that some degree of overfitting can be prevented\n",
    "# verbose is set to true so that all steps of training can be displayed\n",
    "est = MLPClassifier(activation='relu', \\\n",
    "                    hidden_layer_sizes=(100,85),\\\n",
    "                    solver='adam',\\\n",
    "                    learning_rate='adaptive',\\\n",
    "                    max_iter=2000,\\\n",
    "                    learning_rate_init=0.0001,\\\n",
    "                    early_stopping=True,\\\n",
    "                    tol=0.000001,\\\n",
    "                    verbose=True,\\\n",
    "                    alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:55:34.464183Z",
     "start_time": "2020-01-20T20:53:40.672644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Network MLP-Classifier...\n",
      "Iteration 1, loss = 0.81464265\n",
      "Validation score: 0.912826\n",
      "Iteration 2, loss = 0.42569925\n",
      "Validation score: 0.917051\n",
      "Iteration 3, loss = 0.38669795\n",
      "Validation score: 0.919634\n",
      "Iteration 4, loss = 0.36450521\n",
      "Validation score: 0.922529\n",
      "Iteration 5, loss = 0.35019385\n",
      "Validation score: 0.926051\n",
      "Iteration 6, loss = 0.34008885\n",
      "Validation score: 0.928398\n",
      "Iteration 7, loss = 0.33246543\n",
      "Validation score: 0.930902\n",
      "Iteration 8, loss = 0.32694455\n",
      "Validation score: 0.928946\n",
      "Iteration 9, loss = 0.32222512\n",
      "Validation score: 0.931137\n",
      "Iteration 10, loss = 0.31833392\n",
      "Validation score: 0.931450\n",
      "Iteration 11, loss = 0.31510674\n",
      "Validation score: 0.931998\n",
      "Iteration 12, loss = 0.31194218\n",
      "Validation score: 0.931920\n",
      "Iteration 13, loss = 0.30915078\n",
      "Validation score: 0.931763\n",
      "Iteration 14, loss = 0.30669303\n",
      "Validation score: 0.931841\n",
      "Iteration 15, loss = 0.30454031\n",
      "Validation score: 0.931685\n",
      "Iteration 16, loss = 0.30244810\n",
      "Validation score: 0.932780\n",
      "Iteration 17, loss = 0.30048881\n",
      "Validation score: 0.933328\n",
      "Iteration 18, loss = 0.29864367\n",
      "Validation score: 0.932937\n",
      "Iteration 19, loss = 0.29713906\n",
      "Validation score: 0.934580\n",
      "Iteration 20, loss = 0.29578552\n",
      "Validation score: 0.934737\n",
      "Iteration 21, loss = 0.29417779\n",
      "Validation score: 0.934815\n",
      "Iteration 22, loss = 0.29303792\n",
      "Validation score: 0.934267\n",
      "Iteration 23, loss = 0.29159450\n",
      "Validation score: 0.934815\n",
      "Iteration 24, loss = 0.29054444\n",
      "Validation score: 0.935128\n",
      "Iteration 25, loss = 0.28928409\n",
      "Validation score: 0.935206\n",
      "Iteration 26, loss = 0.28836118\n",
      "Validation score: 0.935989\n",
      "Iteration 27, loss = 0.28735696\n",
      "Validation score: 0.936302\n",
      "Iteration 28, loss = 0.28629365\n",
      "Validation score: 0.935754\n",
      "Iteration 29, loss = 0.28550795\n",
      "Validation score: 0.936458\n",
      "Iteration 30, loss = 0.28443055\n",
      "Validation score: 0.935441\n",
      "Iteration 31, loss = 0.28369189\n",
      "Validation score: 0.936223\n",
      "Iteration 32, loss = 0.28286233\n",
      "Validation score: 0.936223\n",
      "Iteration 33, loss = 0.28201090\n",
      "Validation score: 0.935989\n",
      "Iteration 34, loss = 0.28134604\n",
      "Validation score: 0.935519\n",
      "Iteration 35, loss = 0.28057694\n",
      "Validation score: 0.935676\n",
      "Iteration 36, loss = 0.27977264\n",
      "Validation score: 0.935128\n",
      "Iteration 37, loss = 0.27906473\n",
      "Validation score: 0.935910\n",
      "Iteration 38, loss = 0.27829080\n",
      "Validation score: 0.935206\n",
      "Iteration 39, loss = 0.27771099\n",
      "Validation score: 0.935597\n",
      "Iteration 40, loss = 0.27708383\n",
      "Validation score: 0.936615\n",
      "Iteration 41, loss = 0.27635315\n",
      "Validation score: 0.935910\n",
      "Iteration 42, loss = 0.27590810\n",
      "Validation score: 0.935050\n",
      "Iteration 43, loss = 0.27519121\n",
      "Validation score: 0.935206\n",
      "Iteration 44, loss = 0.27479944\n",
      "Validation score: 0.936928\n",
      "Iteration 45, loss = 0.27415946\n",
      "Validation score: 0.936771\n",
      "Iteration 46, loss = 0.27343152\n",
      "Validation score: 0.936145\n",
      "Iteration 47, loss = 0.27319290\n",
      "Validation score: 0.936223\n",
      "Iteration 48, loss = 0.27261845\n",
      "Validation score: 0.935910\n",
      "Iteration 49, loss = 0.27183607\n",
      "Validation score: 0.936615\n",
      "Iteration 50, loss = 0.27141678\n",
      "Validation score: 0.936067\n",
      "Iteration 51, loss = 0.27082863\n",
      "Validation score: 0.937006\n",
      "Iteration 52, loss = 0.27048725\n",
      "Validation score: 0.937397\n",
      "Iteration 53, loss = 0.27023376\n",
      "Validation score: 0.936458\n",
      "Iteration 54, loss = 0.26949459\n",
      "Validation score: 0.935832\n",
      "Iteration 55, loss = 0.26929600\n",
      "Validation score: 0.935676\n",
      "Iteration 56, loss = 0.26860782\n",
      "Validation score: 0.936771\n",
      "Iteration 57, loss = 0.26799008\n",
      "Validation score: 0.936928\n",
      "Iteration 58, loss = 0.26769125\n",
      "Validation score: 0.935128\n",
      "Iteration 59, loss = 0.26738703\n",
      "Validation score: 0.937084\n",
      "Iteration 60, loss = 0.26700033\n",
      "Validation score: 0.936693\n",
      "Iteration 61, loss = 0.26648259\n",
      "Validation score: 0.937006\n",
      "Iteration 62, loss = 0.26626237\n",
      "Validation score: 0.937163\n",
      "Iteration 63, loss = 0.26564696\n",
      "Validation score: 0.935989\n",
      "Validation score did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 85), learning_rate='adaptive',\n",
      "              learning_rate_init=0.0001, max_fun=15000, max_iter=2000,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=1e-06, validation_fraction=0.1, verbose=True,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Neural Network MLP-Classifier...\")\n",
    "nnc = est.fit(X, y)\n",
    "print(nnc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:56:04.707493Z",
     "start_time": "2020-01-20T20:56:03.823330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data are:  0.9395940327708486\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy on training data\n",
    "print(\"Accuracy on Training Data are: \", accuracy_score(nnc.predict(X_train), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:56:07.246087Z",
     "start_time": "2020-01-20T20:56:07.076439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data are:  0.9393903822827405\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy on testing data \n",
    "print(\"Accuracy on Training Data are: \", accuracy_score(nnc.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:56:11.048261Z",
     "start_time": "2020-01-20T20:56:09.863097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97    115892\n",
      "           1       0.67      0.67      0.67      3332\n",
      "           2       0.61      0.29      0.40       102\n",
      "           3       0.66      0.40      0.50       677\n",
      "           4       0.55      0.50      0.52       563\n",
      "           5       0.62      0.22      0.33       238\n",
      "           6       0.57      0.30      0.39        87\n",
      "           7       0.61      0.51      0.56       424\n",
      "           8       0.73      0.45      0.56       353\n",
      "           9       0.72      0.55      0.62       477\n",
      "          10       0.70      0.63      0.67      1480\n",
      "          11       0.75      0.59      0.66       400\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        38\n",
      "          14       0.62      0.19      0.29       292\n",
      "          15       0.71      0.62      0.67       554\n",
      "          16       0.64      0.47      0.54       152\n",
      "          17       0.00      0.00      0.00        47\n",
      "          18       0.58      0.50      0.54       517\n",
      "          19       0.68      0.54      0.60       488\n",
      "          20       0.00      0.00      0.00        16\n",
      "          21       0.00      0.00      0.00        13\n",
      "          22       0.71      0.31      0.43        71\n",
      "          23       0.71      0.25      0.37       479\n",
      "          24       0.71      0.53      0.61      1025\n",
      "          25       0.86      0.12      0.20        52\n",
      "\n",
      "    accuracy                           0.94    127782\n",
      "   macro avg       0.55      0.37      0.43    127782\n",
      "weighted avg       0.93      0.94      0.93    127782\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Check classification (confusion matrix) report on overall data set\n",
    "print(classification_report(y, nnc.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:56:52.734235Z",
     "start_time": "2020-01-20T20:56:52.721813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./nnc.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model into a file so that it can be used later on without re-training the model\n",
    "joblib.dump(nnc, './nnc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T20:57:45.408633Z",
     "start_time": "2020-01-20T20:57:45.230591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Testing Data are:  0.9393903822827405\n"
     ]
    }
   ],
   "source": [
    "# Load the classification model\n",
    "loaded_model = joblib.load('./nnc.pkl')\n",
    "# Predict and score the data set with the loaded model\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(\"Accuracy on Testing Data are: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook I went through major steps to demonstrate how a NLP data science project can be implemented. \n",
    "\n",
    "During this project implementation, I used pandas, gensim and scikit-learn libraries running on a local machine. That is, the running time can be an exponential growth on the size of training data set. This issue can be addressed by using multiprocessing or dask libraries to enhance parallel processing.\n",
    "\n",
    "Another scalable solution for this problem is to use Spark (with pyspark library) on a Hadoop cluster (a group of multiple servers). However, it also means the NLP models will be depended on the spark distributed environment. So, this approach is probably only applicable or feasible when a big data volume of training data set needs to be considered."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371.141px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
